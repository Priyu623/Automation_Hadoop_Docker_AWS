# Automation_Hadoop_Docker_AWS
I have done the following things as follows :-

Hadoop

Run any Linux Command Locally & Remotely

Configure WebServer on Local OS , Remote OS and AWS Cloud

Configure and start Namenode on Local OS and AWS Cloud

Configure and start Datanode on Local OS and AWS Cloud

Created Virtual Group

Created Logical Volume

Attach More hard-disks to Virtual group dynamically

Increase Partition Size Dynamically


AWS Cloud 


Created & Deleted Key Pair

Created & Deleted Security Group

Adding Ingress rules to existing Security group

Launch Instance 

Created EBS volume

Attached EBS volume to Ec2 Instance 

Configured WebServer

Created stating partition and mount /var/www/html folder on EBS volume

Created S3 bucket accessible to public

Inserted the object inside s3 bucket which is accessible by everyone

Created Cloudfront distribution providing S3 as origin

Delete object inside S3 bucket 

Deleted S3 bucket

Stop , Start and terminate Ec2 Instance


Docker


Pull Image from Docker hub

launch Container

Click to know number of Container running and docker Images in OS

Inspect docker container

Remove docker Images from OS

Start and stop docker Container

Delete Single or all docker container

Configure Webserver inside docker container


Machine Learning

Predict output from Data Set
